{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using Conditional Random Fields\n",
    "\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "\n",
    "The objective is to build an entity recognition model to predict various entities/phrases from unstructured text. \n",
    "\n",
    "The data consists of a set of sentences (sequences) each of which contains a series of words (e.g., 'Zamibian', 'officials') and the respective IOB labels (e.g., 'B-gpe', 'O').  \n",
    "\n",
    "Quick peek of the dataset below\n",
    "\n",
    "\n",
    "| sentence_id  | word       | Tag     \n",
    "|--------------|------------|-------\n",
    "| 704          | Zamibian   | B-gpe   \n",
    "| 704          | officials  | O       \n",
    "| 704          | say        | O    \n",
    "\n",
    "\n",
    "The entities to be recognized are as follow  \n",
    "\n",
    "nat -> Natural Phenomenon  \n",
    "gpe -> Geopolitical  \n",
    "tim -> Time   \n",
    "geo -> Geographical   \n",
    "org -> Organization  \n",
    "per -> Person  \n",
    "art -> Artifact  \n",
    "eve -> Event  \n",
    "\n",
    "The target variable to predict is Tag and it follows the below convention.  \n",
    "B-{ } : Beginning of an entity phrase  \n",
    "I-{ } : Inside an entity phrase  \n",
    "O     : Outside      \n",
    "\n",
    "We will predict the entities using **Conditional Random Fields**.\n",
    "  \n",
    "Conditional Random Field is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs.\n",
    "\n",
    "\n",
    "### Environment set-up\n",
    "\n",
    "Install the libraries defined in `requirements.txt` file using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.16.5)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (0.3.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.4.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->-r requirements.txt (line 3)) (4.43.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->-r requirements.txt (line 3)) (0.8.6)\n",
      "Requirement already satisfied: six in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->-r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\vinubalan\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->-r requirements.txt (line 3)) (0.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vinubalan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vinubalan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option(\"display.max_colwidth\", 30)\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "\n",
    "# Text feature extraction - Custom functions defined in utils.py\n",
    "from utils import sent2features\n",
    "\n",
    "# Modeling Algorithm\n",
    "import sklearn_crfsuite\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "\n",
    "# Saving and loading Model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46464, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704</td>\n",
       "      <td>Zambian</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704</td>\n",
       "      <td>officials</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704</td>\n",
       "      <td>say</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704</td>\n",
       "      <td>reports</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id       word    Tag\n",
       "0          704    Zambian  B-gpe\n",
       "1          704  officials      O\n",
       "2          704        say      O\n",
       "3          704    reports      O\n",
       "4          704       that      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train_new.txt\", sep = \"\\s+\",quoting=csv.QUOTE_NONE)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tag.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "df_new =  df.head(100).drop(columns = ['sentence_id','Tag'])\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(df_new.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zambian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>officials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word\n",
       "0    Zambian\n",
       "1  officials\n",
       "2        say\n",
       "3    reports\n",
       "4       that"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.word.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentences : 2099 , Unique words :7310\n",
      "\n",
      "Proportion of Tags\n",
      "O        84.92\n",
      "B-geo     3.11\n",
      "I-per     1.90\n",
      "B-org     1.87\n",
      "B-gpe     1.87\n",
      "B-tim     1.75\n",
      "B-per     1.67\n",
      "I-org     1.43\n",
      "I-geo     0.61\n",
      "I-tim     0.49\n",
      "B-art     0.09\n",
      "B-eve     0.08\n",
      "I-eve     0.06\n",
      "I-art     0.06\n",
      "I-gpe     0.05\n",
      "B-nat     0.03\n",
      "I-nat     0.01\n",
      "Name: Tag, dtype: float64\n",
      "\n",
      "Proportion of Tags excluding'O'\n",
      "B-geo    20.62\n",
      "I-per    12.61\n",
      "B-org    12.43\n",
      "B-gpe    12.39\n",
      "B-tim    11.57\n",
      "B-per    11.07\n",
      "I-org     9.47\n",
      "I-geo     4.05\n",
      "I-tim     3.22\n",
      "B-art     0.57\n",
      "B-eve     0.51\n",
      "I-eve     0.43\n",
      "I-art     0.41\n",
      "I-gpe     0.36\n",
      "B-nat     0.20\n",
      "I-nat     0.07\n",
      "Name: Tag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique sentences : {} , Unique words :{}\\n\".format(df.sentence_id.nunique(), df.word.nunique()))\n",
    "\n",
    "print(\"Proportion of Tags\")\n",
    "print(np.round(df.Tag.value_counts(normalize = True),4)*100)\n",
    "print(\"\\nProportion of Tags excluding'O'\")\n",
    "print(np.round(df[df.Tag != 'O'].Tag.value_counts(normalize = True),4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of 'O' is ~85% and outweighs all other tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "For each sentence, we will retrieve tokens and their corresponding tags using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence Words Sequence\n",
      " ['Zambian', 'officials', 'say', 'reports', 'that', 'President', 'Levy', 'Mwanawasa', 'has', 'died', 'are', 'FALSE', '.']\n",
      "First Sentence Tags\n",
      " ['B-gpe', 'O', 'O', 'O', 'O', 'B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "y = []\n",
    "\n",
    "# for each sentence ID, extract\n",
    "# 1. List of words in the sentence\n",
    "# 2. List of corresponding tags to the words\n",
    "\n",
    "for i in df.sentence_id.unique():\n",
    "    sentences.append(df[df.sentence_id == i].word.tolist())\n",
    "    y.append(df[df.sentence_id == i].Tag.tolist())\n",
    "\n",
    "print('First Sentence Words Sequence\\n', sentences[0])\n",
    "print('First Sentence Tags\\n', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction\n",
    "\n",
    "In a sequence of sentence, for a word at position [t], the characteristics of the word can be expressed by its surrounding words in positions [t-1] and [t+1]\n",
    "\n",
    "We will therefore enhance the feature set by taking account of features of surrounding words.\n",
    "\n",
    "For each word in the sentence, the following features are extracted\n",
    "\n",
    "* **Word in Lowercase**    \n",
    "Word converted to lowercase  \n",
    "Features : word.lower, +1:word.lower, -1:word.lower  \n",
    "<br />  \n",
    "* **Word Shape**    \n",
    "Indicates whether word is number, uppercase, lowercase, capitalized, camelcase, mixedcase, wildcard, endingdot, abbrevation, contains-hyphen  \n",
    "Features : word.shape, +1:word.shape, -1:word.shape  \n",
    "<br />  \n",
    "* **Stemmed Word**  \n",
    "Normalized word using stemming. PorterStemmer api from nltk library used.    \n",
    "Features : word.stem, +1:word.stem, -1:word.stem   \n",
    "<br />  \n",
    "* **Lemmatized Word**    \n",
    "Normalized word using Lemmatization. WordLemmatizer api from nltk library used.  \n",
    "Features : word.lemma, +1:word.lemma, -1:word.lemma  \n",
    "<br />  \n",
    "* **Parts of Speech**     \n",
    "Extracts parts of speech of the word. pos_tag api from nltk library used.    \n",
    "Features : word.pos, +1:word.pos, -1:word.pos  \n",
    "Related features : word.pos[:2], +1:word.pos[:2], -1:word.pos[:2] - Implies last two characters of POS  \n",
    "<br />   \n",
    "* **Beginning or End of Sentence**  \n",
    "Indicates whether word is in beginning of sentence or end of sentence  \n",
    "Features : BOS, EOS  \n",
    "\n",
    "\n",
    "_Note: Features of previous word is denoted with a prefix '-1' and features of next word with prefix '+1'_ \n",
    "\n",
    "The features are stored in format required by sklearn-crfsuite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "\n",
    "print('Features of words from First Sentence below')\n",
    "pd.DataFrame(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and Selection\n",
    "\n",
    "Now we will train a Linear-chain Conditional Random Fields model using `sklearn_crfsuite.CRF`\n",
    "\n",
    "Gradient descent using the L-BFGS method (default) is used with regularization.\n",
    "\n",
    "Instead of training the model with default regularization parameters (c1 = 0 and c2 = 1), we will search for optimal values using `sklearn.model_selection.RandomizedSearchCV` with 10 iterations\n",
    "\n",
    "5-fold cross validation used to assess performace of the model across combinations of c1 and c2 values. \n",
    "\n",
    "With the best found parameters, `RandomizedSearchCV` refits on the whole dataset by default.\n",
    "<br />  \n",
    "**Evaluation Metric for Model Selection**\n",
    "\n",
    "NER can be viewed as multi-class classification problem, where the IOB Tag `O` comprises of ~85% of all labels\n",
    "\n",
    "Predicting `O` as `O` is of least importance to us and it can be considered equivalent to predicting True Negatives in binary imbalance classification problem.\n",
    "\n",
    "True Positives, False Negatives and False Positives are of prime importance for each of the labels and therefore we will use F1 score as evaluation metric.  \n",
    "\n",
    "In a multiclass setting, the average parameter in the F1 score function needs to be additionally selected from the choices below\n",
    "\n",
    "* `micro` : calculates the F1 directly by using the global number of TP, FN and FP  \n",
    "\n",
    "   $F1_{class1 + class2 +...+classN}$\n",
    "<br />\n",
    "\n",
    "* `macro` : calculates the F1 separated by class but not using weights for the aggregation   \n",
    "   $F1_{class1}$ + $F1_{class2}$ +...+ $F1_{classN}$\n",
    "<br />\n",
    "\n",
    "* `weighted` :calculates F1 score for each class independently. Final F1 score is calculated using weights that depends on the number of true labels of each class:  \n",
    "\n",
    "    $F1_{class1}$ ∗ $W_{class1}$ + $F1_{class2}$ ∗ $W_{class2}$ +...+ $F1_{classN}$ ∗ $W_{classN}$    \n",
    "<br /> \n",
    "\n",
    "\n",
    "**As `macro` results in bigger penalisation when model does not perform well with the minority classes, we will choose F1 score average method macro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Random Field Classifier\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=50,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# Parameter Space for L1 (c1) and L2 (c2) regularization\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# Metric for evaluation - F1 Macro\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='macro')\n",
    "\n",
    "# Random Search with 5 fold CV and 10 Iterations\n",
    "rs = RandomizedSearchCV(crf, \n",
    "                        params_space,\n",
    "                        cv=5,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=10,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and View CV results\n",
    "cvresults = pd.DataFrame(rs.cv_results_)\n",
    "print(cvresults.shape)\n",
    "cvresults.sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Best Parameters\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved best F1 Score (macro) of 0.5648 at c1 value 0.21 and c2-value  0.01\n",
    "\n",
    "\n",
    "Final estimator is refitted by `RandomizedSearchCV` on the entire dataset (X) with the above regulatization values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Model\n",
    "\n",
    "The best estimator is extracted and saved in folder `model` for future scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model/ner_model.pickle\"\n",
    "pickle.dump(rs.best_estimator_, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope for further work\n",
    "\n",
    "\n",
    "Spacy and few other python libraries have open-sourced pre-trained NER models. These models can be further custom trained with our IOB tags.   \n",
    "\n",
    "\n",
    "Code example  \n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "* [Named Entity Recognition and Classification](https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2)\n",
    "* [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
